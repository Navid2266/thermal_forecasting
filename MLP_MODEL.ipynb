{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f495d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Optional\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e82ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"postgres\", \n",
    "        password=\"password\",\n",
    "        database=\"postgres\"\n",
    "    )\n",
    "    query = \"\"\"\n",
    "    SELECT timestamp_10s, avg_indoor_temperature, avg_indoor_humidity, \n",
    "           avg_exhaust_temperature, heating_status, solar_radiation, outdoor_temp \n",
    "    FROM apartment_11_10s \n",
    "    ORDER BY timestamp_10s\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    df['timestamp_10s'] = pd.to_datetime(df['timestamp_10s'])\n",
    "    df.set_index('timestamp_10s', inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3f824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize(\"UTC\")\n",
    "    df.index = df.index.tz_convert(\"Asia/Tehran\")\n",
    "\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek  \n",
    "\n",
    "    # Thursday (3) and Friday (4)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([3, 4]).astype(int)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "462b72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heating_duration(df, time_threshold='5min'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['time_diff'] = df.index.to_series().diff().dt.total_seconds()\n",
    "    df['is_gap'] = df['time_diff'] > pd.Timedelta(time_threshold).total_seconds()\n",
    "    \n",
    "    # Reset duration at gaps\n",
    "    duration = 0\n",
    "    durations = []\n",
    "    prev_status = None\n",
    "    \n",
    "    for i, (status, is_gap) in enumerate(zip(df['heating_status'], df['is_gap'])):\n",
    "        if i == 0 or is_gap:\n",
    "            duration = 0\n",
    "        elif status == prev_status:\n",
    "            duration += df['time_diff'].iloc[i]\n",
    "        else:\n",
    "            duration = 0\n",
    "            \n",
    "        durations.append(duration)\n",
    "        prev_status = status\n",
    "    \n",
    "    df['heating_duration_sec'] = durations\n",
    "    df['heating_duration_min'] = df['heating_duration_sec'] / 60\n",
    "    return df.drop(columns=['time_diff', 'is_gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c6527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_500825/1890870290.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "class TimeAwareSeriesDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 cont_features: List[str],\n",
    "                 bin_features: List[str],\n",
    "                 target_col: str = 'avg_indoor_temperature',\n",
    "                 history_hours: float = 3,  \n",
    "                 pred_horizon_hours: float = 2, \n",
    "                 max_gap_minutes: float = 15,  # Maximum allowed gap in minutes\n",
    "                 scaler: Optional[MinMaxScaler] = None):\n",
    "        \"\"\"\n",
    "            cont_features: List of continuous feature names\n",
    "            bin_features: List of binary feature names\n",
    "            target_col: Name of target column\n",
    "            history_hours: Length of history window in hours\n",
    "            pred_horizon_hours: Prediction horizon in hours\n",
    "            max_gap_minutes: Maximum allowed gap within a window (minutes)\n",
    "            scaler: Pre-fit scaler or None to create new\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.cont_features = cont_features\n",
    "        self.bin_features = bin_features\n",
    "        self.features = cont_features + bin_features\n",
    "        self.target_col = target_col\n",
    "        self.max_gap_minutes = max_gap_minutes\n",
    "        \n",
    "        # Calculate time differences in minutes\n",
    "        self.time_deltas = self.df.index.to_series().diff().dt.total_seconds().div(60).values\n",
    "        \n",
    "        # Convert time windows from hours to data points\n",
    "        self.median_interval = np.median(self.time_deltas[1:])  # Skip first NaN\n",
    "        self.history_length = int(history_hours * 60 / self.median_interval)\n",
    "        self.pred_horizon = int(pred_horizon_hours * 60 / self.median_interval)\n",
    "        \n",
    "        # Normalization\n",
    "        self.scaler = scaler or MinMaxScaler()\n",
    "        if scaler is None:\n",
    "            self.df[cont_features] = self.scaler.fit_transform(self.df[cont_features])\n",
    "        else:\n",
    "            self.df[cont_features] = self.scaler.transform(self.df[cont_features])\n",
    "        \n",
    "        # Create shifted target column\n",
    "        self.df['target_temp'] = self.df[target_col].shift(-self.pred_horizon)\n",
    "        self.df.dropna(subset=['target_temp'], inplace=True)\n",
    "        \n",
    "        # Pre-compute valid indices\n",
    "        self.valid_idx = self._compute_valid_indices()\n",
    "    \n",
    "    def _compute_valid_indices(self) -> List[int]:\n",
    "        \"\"\"Find indices where the time gaps are within tolerance\"\"\"\n",
    "        valid_idx = []\n",
    "        total_length = self.history_length + self.pred_horizon\n",
    "        \n",
    "        for i in range(len(self.df) - total_length + 1):\n",
    "            # Check gaps in history window\n",
    "            history_gaps = self.time_deltas[i+1 : i+self.history_length]\n",
    "            \n",
    "            # Check gap between history end and target\n",
    "            prediction_gaps = self.time_deltas[i+self.history_length : i+total_length]\n",
    "            \n",
    "            if (np.all(history_gaps <= self.max_gap_minutes) and \n",
    "                np.all(prediction_gaps <= self.max_gap_minutes)):\n",
    "                valid_idx.append(i)\n",
    "                \n",
    "        return valid_idx\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_idx)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        i = self.valid_idx[idx]\n",
    "        \n",
    "        # Features (history window)\n",
    "        X = self.df[self.features].iloc[i:i+self.history_length].values.astype(np.float32)\n",
    "        \n",
    "        # Target (single point at prediction horizon)\n",
    "        y = self.df['target_temp'].iloc[i+self.history_length-1].astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(X), torch.tensor(y).unsqueeze(0)\n",
    "\n",
    "# Feature lists\n",
    "binary_features = ['heating_status', 'is_weekend']\n",
    "continuous_features = [\n",
    "    'avg_indoor_temperature', 'avg_indoor_humidity',\n",
    "    'avg_exhaust_temperature', 'solar_radiation', \n",
    "    'outdoor_temp', 'hour_sin', 'hour_cos', 'heating_duration_min'\n",
    "]\n",
    "\n",
    "# Modified data loading and preprocessing\n",
    "def load_and_preprocess_data():\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    \n",
    "    # Filter dates\n",
    "    start_date = '2025-01-16' \n",
    "    end_date = '2025-01-30'\n",
    "    df = df.loc[start_date:end_date].copy()\n",
    "    \n",
    "    # Add features\n",
    "    df = add_time_features(df)\n",
    "    df = add_heating_duration(df)\n",
    "    \n",
    "    # Train-test split (time-based)\n",
    "    split_point = int(0.8 * len(df))\n",
    "    df_train = df.iloc[:split_point].copy()\n",
    "    df_test = df.iloc[split_point:].copy()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "# Create datasets\n",
    "df_train, df_test = load_and_preprocess_data()\n",
    "\n",
    "train_dataset = TimeAwareSeriesDataset(\n",
    "    df_train,\n",
    "    continuous_features,\n",
    "    binary_features,\n",
    "    history_hours=3, \n",
    "    pred_horizon_hours=2, \n",
    "    max_gap_minutes=15 \n",
    ")\n",
    "\n",
    "test_dataset = TimeAwareSeriesDataset(\n",
    "    df_test,\n",
    "    continuous_features,\n",
    "    binary_features,\n",
    "    scaler=train_dataset.scaler,  # Use same scaler\n",
    "    history_hours=3,\n",
    "    pred_horizon_hours=2,\n",
    "    max_gap_minutes=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef9d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "TimeSeriesMLP(\n",
      "  (temporal_compressor): Sequential(\n",
      "    (0): Linear(in_features=1080, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total parameters: 274,497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amashayekh/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m val_maes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m--> 100\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, criterion, device)\n\u001b[1;32m    101\u001b[0m     val_loss, val_mae \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion, device)\n\u001b[1;32m    103\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[1;32m     36\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     38\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    237\u001b[0m         group,\n\u001b[1;32m    238\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         state_steps,\n\u001b[1;32m    244\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     adam(\n\u001b[1;32m    247\u001b[0m         params_with_grad,\n\u001b[1;32m    248\u001b[0m         grads,\n\u001b[1;32m    249\u001b[0m         exp_avgs,\n\u001b[1;32m    250\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    251\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    252\u001b[0m         state_steps,\n\u001b[1;32m    253\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    254\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    255\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    256\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    257\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    258\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    259\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    260\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    264\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    265\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    266\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    267\u001b[0m         decoupled_weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoupled_weight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 933\u001b[0m func(\n\u001b[1;32m    934\u001b[0m     params,\n\u001b[1;32m    935\u001b[0m     grads,\n\u001b[1;32m    936\u001b[0m     exp_avgs,\n\u001b[1;32m    937\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    938\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    939\u001b[0m     state_steps,\n\u001b[1;32m    940\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    941\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    942\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    943\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    944\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    945\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    946\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    947\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    948\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    949\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    950\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    951\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    952\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[1;32m    953\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:525\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    523\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 525\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    527\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class TimeSeriesMLP(nn.Module):\n",
    "    def __init__(self, input_features=10, seq_length=1080):\n",
    "        super().__init__()\n",
    "        # Temporal compression (1080 -> 64)\n",
    "        self.temporal_compressor = nn.Sequential(\n",
    "            nn.Linear(seq_length, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Feature processing\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(64 * input_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len=1080, features=10)\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, seq_len)\n",
    "        x = self.temporal_compressor(x)  # (batch, features, 64)\n",
    "        x = x.reshape(x.size(0), -1)  # (batch, features*64)\n",
    "        return self.net(x)\n",
    "\n",
    "#Define Training and Evaluation Functions\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "            all_preds.extend(outputs.cpu().numpy().flatten())\n",
    "            all_true.extend(y.cpu().numpy().flatten())\n",
    "    mae = mean_absolute_error(all_true, all_preds)\n",
    "    return total_loss / len(loader.dataset), mae\n",
    "\n",
    "#Initialize Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TimeSeriesMLP(input_features=len(train_dataset.features), \n",
    "                     seq_length=train_dataset.history_length).to(device)\n",
    "print(f\"Model architecture:\\n{model}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "#Training Setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4,        # Parallel data loading\n",
    "    pin_memory=True,      # Faster GPU transfer\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_mae = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_maes.append(val_mae)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:>3} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            'metrics': {\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'val_maes': val_maes\n",
    "            }\n",
    "        }, 'saved_models/best_model.pth')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'saved_models/final_model.pth')\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "final_train_loss, final_train_mae = evaluate(model, train_loader, criterion, device)\n",
    "final_val_loss, final_val_mae = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nFinal Performance:\")\n",
    "print(f\"Train Loss: {final_train_loss:.4f} | MAE: {final_train_mae:.4f}\")\n",
    "print(f\"Val Loss: {final_val_loss:.4f} | MAE: {final_val_mae:.4f}\")\n",
    "\n",
    "# 7. Prediction Example\n",
    "def predict(model, dataset, idx, scaler):\n",
    "    model.eval()\n",
    "    X, y = dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        pred = model(X.unsqueeze(0).to(device)).cpu().item()\n",
    "    \n",
    "    # Inverse transform if target was scaled\n",
    "    if dataset.target_col in dataset.cont_features:\n",
    "        dummy = np.zeros(len(dataset.cont_features))\n",
    "        dummy[dataset.cont_features.index(dataset.target_col)] = pred\n",
    "        pred = scaler.inverse_transform([dummy])[0][dataset.cont_features.index(dataset.target_col)]\n",
    "        \n",
    "        dummy[dataset.cont_features.index(dataset.target_col)] = y.item()\n",
    "        y = scaler.inverse_transform([dummy])[0][dataset.cont_features.index(dataset.target_col)]\n",
    "    \n",
    "    print(f\"\\nSample {idx} Prediction:\")\n",
    "    print(f\"True: {y:.2f} | Pred: {pred:.2f} | Error: {abs(y-pred):.2f}\")\n",
    "\n",
    "# Test prediction\n",
    "predict(model, test_dataset, 0, train_dataset.scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b7303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load Best Model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "final_train_loss, final_train_mae = evaluate(model, train_loader, criterion, device)\n",
    "final_val_loss, final_val_mae = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"\\nFinal Performance:\")\n",
    "print(f\"Train Loss: {final_train_loss:.4f} | MAE: {final_train_mae:.4f}\")\n",
    "print(f\"Val Loss: {final_val_loss:.4f} | MAE: {final_val_mae:.4f}\")\n",
    "\n",
    "#Prediction Example\n",
    "def predict(model, dataset, idx, scaler):\n",
    "    model.eval()\n",
    "    X, y = dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        pred = model(X.unsqueeze(0).to(device)).cpu().item()\n",
    "    \n",
    "    # Inverse transform if target was scaled\n",
    "    if dataset.target_col in dataset.cont_features:\n",
    "        dummy = np.zeros(len(dataset.cont_features))\n",
    "        dummy[dataset.cont_features.index(dataset.target_col)] = pred\n",
    "        pred = scaler.inverse_transform([dummy])[0][dataset.cont_features.index(dataset.target_col)]\n",
    "        \n",
    "        dummy[dataset.cont_features.index(dataset.target_col)] = y.item()\n",
    "        y = scaler.inverse_transform([dummy])[0][dataset.cont_features.index(dataset.target_col)]\n",
    "    \n",
    "    print(f\"\\nSample {idx} Prediction:\")\n",
    "    print(f\"True: {y:.2f} | Pred: {pred:.2f} | Error: {abs(y-pred):.2f}\")\n",
    "\n",
    "# Test prediction\n",
    "predict(model, test_dataset, 0, train_dataset.scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
