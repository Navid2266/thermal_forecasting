{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a687231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98292d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"postgres\", \n",
    "        password=\"password\",\n",
    "        database=\"postgres\"\n",
    "    )\n",
    "    query = \"\"\"\n",
    "    SELECT timestamp_10s, avg_indoor_temperature, avg_indoor_humidity, \n",
    "           avg_exhaust_temperature, heating_status, solar_radiation, outdoor_temp \n",
    "    FROM apartment_11_10s \n",
    "    ORDER BY timestamp_10s\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    df['timestamp_10s'] = pd.to_datetime(df['timestamp_10s'])\n",
    "    df.set_index('timestamp_10s', inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4712002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_406775/1890870290.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 5 continuous segments:\n",
      "Segment 1: 2025-01-16 14:25:40 to 2025-02-05 05:36:30 (Duration: 19 days 15:10:50)\n",
      "Segment 2: 2025-02-05 07:05:40 to 2025-02-11 16:42:10 (Duration: 6 days 09:36:30)\n",
      "Segment 3: 2025-02-11 20:19:30 to 2025-02-17 16:09:00 (Duration: 5 days 19:49:30)\n",
      "Segment 4: 2025-02-18 09:10:30 to 2025-03-04 06:32:00 (Duration: 13 days 21:21:30)\n",
      "Segment 5: 2025-03-05 11:47:20 to 2025-03-22 18:18:10 (Duration: 17 days 06:30:50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def get_contiguous_segments():\n",
    "\n",
    "        \n",
    "    df = load_data()\n",
    "    \n",
    "    gap_threshold = timedelta(hours=1)  \n",
    "    time_diffs = df.index.to_series().diff()\n",
    "    gaps = time_diffs > gap_threshold\n",
    "    gap_indices = df.index[gaps]\n",
    "\n",
    "    segments = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for gap_idx in gap_indices:\n",
    "        end_idx = df.index.get_loc(gap_idx) - 1\n",
    "        if end_idx > start_idx:\n",
    "            segments.append(df.iloc[start_idx:end_idx])\n",
    "        start_idx = df.index.get_loc(gap_idx) + 1\n",
    "\n",
    "    segments.append(df.iloc[start_idx:])\n",
    "\n",
    "    print(f\"Split into {len(segments)} continuous segments:\")\n",
    "    for i, seg in enumerate(segments):\n",
    "        duration = seg.index[-1] - seg.index[0]\n",
    "        print(f\"Segment {i+1}: {seg.index[0]} to {seg.index[-1]} (Duration: {duration})\")\n",
    "        \n",
    "    return segments\n",
    "\n",
    "segments = get_contiguous_segments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e283229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize(\"UTC\")\n",
    "    df.index = df.index.tz_convert(\"Asia/Tehran\")\n",
    "\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek  # Monday=0\n",
    "\n",
    "    # Thursday (3) and Friday (4)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([3, 4]).astype(int)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b8338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heating_duration(df, time_threshold='5min'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['time_diff'] = df.index.to_series().diff().dt.total_seconds()\n",
    "    df['is_gap'] = df['time_diff'] > pd.Timedelta(time_threshold).total_seconds()\n",
    "    \n",
    "    # Reset duration at gaps\n",
    "    duration = 0\n",
    "    durations = []\n",
    "    prev_status = None\n",
    "    \n",
    "    for i, (status, is_gap) in enumerate(zip(df['heating_status'], df['is_gap'])):\n",
    "        if i == 0 or is_gap:\n",
    "            duration = 0\n",
    "        elif status == prev_status:\n",
    "            duration += df['time_diff'].iloc[i]\n",
    "        else:\n",
    "            duration = 0\n",
    "            \n",
    "        durations.append(duration)\n",
    "        prev_status = status\n",
    "    \n",
    "    df['heating_duration_sec'] = durations\n",
    "    df['heating_duration_min'] = df['heating_duration_sec'] / 60\n",
    "    return df.drop(columns=['time_diff', 'is_gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a59a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, prediction_horizon=2*60*6, history_length=3*60*6):\n",
    "    \n",
    "    df['target_temp'] = df['avg_indoor_temperature'].shift(-prediction_horizon)\n",
    "    df.dropna(subset=['target_temp'], inplace=True)\n",
    "    \n",
    "    # Separate feature types\n",
    "    binary_features = ['heating_status', 'is_weekend']\n",
    "    continuous_features = [\n",
    "        'avg_indoor_temperature', 'avg_indoor_humidity',\n",
    "        'avg_exhaust_temperature', 'solar_radiation', \n",
    "        'outdoor_temp', 'hour_sin', 'hour_cos',\n",
    "        'heating_duration_min'\n",
    "    ]\n",
    "    \n",
    "    # Normalize continuous features\n",
    "    cont_scaler = MinMaxScaler()\n",
    "    df[continuous_features] = cont_scaler.fit_transform(df[continuous_features])\n",
    "    \n",
    "    # Create sequences\n",
    "    def create_sequences(data, targets):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - history_length - prediction_horizon):\n",
    "            X.append(data.iloc[i:i+history_length].values)\n",
    "            y.append(targets.iloc[i+history_length+prediction_horizon-1])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    X, y = create_sequences(df[continuous_features + binary_features], df['target_temp'])\n",
    "    \n",
    "    # Train-test split\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Display sample sequence\n",
    "    sample_idx = 500  # Arbitrary position\n",
    "    print(\"Input features shape:\", X_train[sample_idx].shape)\n",
    "    print(\"First timestep features:\\n\", X_train[sample_idx][0])\n",
    "    print(\"Heating duration values:\", X_train[sample_idx][:, -3])  # 3rd last feature\n",
    "    print(\"Corresponding target:\", y_train[sample_idx])\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), cont_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19947908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(X_train, y_train, X_test, y_test, batch_size=64):\n",
    "\n",
    "    train_data = TensorDataset(\n",
    "        torch.FloatTensor(X_train), \n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    test_data = TensorDataset(\n",
    "        torch.FloatTensor(X_test),\n",
    "        torch.FloatTensor(y_test)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d4e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_380516/3346128653.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "df = add_time_features(df)  \n",
    "df = add_heating_duration(df)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test), scaler = prepare_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
