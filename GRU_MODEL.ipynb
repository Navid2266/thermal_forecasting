{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a687231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 17:30:24.596133: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-17 17:30:27.604235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747490428.141490  380516 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747490428.234261  380516 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747490429.793125  380516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747490429.793164  380516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747490429.793165  380516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747490429.793167  380516 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-17 17:30:29.914684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98292d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        user=\"postgres\", \n",
    "        password=\"password\",\n",
    "        database=\"postgres\"\n",
    "    )\n",
    "    query = \"\"\"\n",
    "    SELECT timestamp_10s, avg_indoor_temperature, avg_indoor_humidity, \n",
    "           avg_exhaust_temperature, heating_status, solar_radiation, outdoor_temp \n",
    "    FROM apartment_11_10s \n",
    "    ORDER BY timestamp_10s\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    # Convert timestamp and set as index\n",
    "    df['timestamp_10s'] = pd.to_datetime(df['timestamp_10s'])\n",
    "    df.set_index('timestamp_10s', inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e283229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df):\n",
    "    # Localize to UTC if naive, then convert to Tehran timezone\n",
    "    if df.index.tz is None:\n",
    "        df.index = df.index.tz_localize(\"UTC\")\n",
    "    df.index = df.index.tz_convert(\"Asia/Tehran\")\n",
    "\n",
    "    # Extract time components\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek  # Monday=0\n",
    "\n",
    "    # Weekend is now Thursday (3) and Friday (4)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([3, 4]).astype(int)\n",
    "\n",
    "    # Cyclical encoding for hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b8338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heating_duration(df, time_threshold='5min'):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Mark gaps (>5min between observations)\n",
    "    df['time_diff'] = df.index.to_series().diff().dt.total_seconds()\n",
    "    df['is_gap'] = df['time_diff'] > pd.Timedelta(time_threshold).total_seconds()\n",
    "    \n",
    "    # Reset duration at gaps\n",
    "    duration = 0\n",
    "    durations = []\n",
    "    prev_status = None\n",
    "    \n",
    "    for i, (status, is_gap) in enumerate(zip(df['heating_status'], df['is_gap'])):\n",
    "        if i == 0 or is_gap:\n",
    "            duration = 0\n",
    "        elif status == prev_status:\n",
    "            duration += df['time_diff'].iloc[i]\n",
    "        else:\n",
    "            duration = 0\n",
    "            \n",
    "        durations.append(duration)\n",
    "        prev_status = status\n",
    "    \n",
    "    df['heating_duration_sec'] = durations\n",
    "    df['heating_duration_min'] = df['heating_duration_sec'] / 60\n",
    "    return df.drop(columns=['time_diff', 'is_gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a59a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, prediction_horizon=2*60*6, history_length=3*60*6):\n",
    "    # Create target\n",
    "    df['target_temp'] = df['avg_indoor_temperature'].shift(-prediction_horizon)\n",
    "    df.dropna(subset=['target_temp'], inplace=True)\n",
    "    \n",
    "    # Separate feature types\n",
    "    binary_features = ['heating_status', 'is_weekend']\n",
    "    continuous_features = [\n",
    "        'avg_indoor_temperature', 'avg_indoor_humidity',\n",
    "        'avg_exhaust_temperature', 'solar_radiation', \n",
    "        'outdoor_temp', 'hour_sin', 'hour_cos',\n",
    "        'heating_duration_min'\n",
    "    ]\n",
    "    \n",
    "    # Normalize continuous features (using MinMax as requested)\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    cont_scaler = MinMaxScaler()\n",
    "    df[continuous_features] = cont_scaler.fit_transform(df[continuous_features])\n",
    "    \n",
    "    # Create sequences\n",
    "    def create_sequences(data, targets):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - history_length - prediction_horizon):\n",
    "            X.append(data.iloc[i:i+history_length].values)\n",
    "            y.append(targets.iloc[i+history_length+prediction_horizon-1])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    X, y = create_sequences(df[continuous_features + binary_features], df['target_temp'])\n",
    "    \n",
    "    # Train-test split\n",
    "    split_idx = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Display sample sequence\n",
    "    sample_idx = 500  # Arbitrary position\n",
    "    print(\"Input features shape:\", X_train[sample_idx].shape)\n",
    "    print(\"First timestep features:\\n\", X_train[sample_idx][0])\n",
    "    print(\"Heating duration values:\", X_train[sample_idx][:, -3])  # 3rd last feature\n",
    "    print(\"Corresponding target:\", y_train[sample_idx])\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test), cont_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19947908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_torch_loaders(X_train, y_train, X_test, y_test, batch_size=64):\n",
    "    # Convert to PyTorch tensors\n",
    "    train_data = TensorDataset(\n",
    "        torch.FloatTensor(X_train), \n",
    "        torch.FloatTensor(y_train)\n",
    "    )\n",
    "    test_data = TensorDataset(\n",
    "        torch.FloatTensor(X_test),\n",
    "        torch.FloatTensor(y_test)\n",
    "    )\n",
    "    \n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total sequences to create: {len(df) - history_length - prediction_horizon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3d4e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_380516/3346128653.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 1. Load and prepare the data\n",
    "df = load_data()\n",
    "\n",
    "# 2. Add REQUIRED features first\n",
    "df = add_time_features(df)  # Creates hour_sin, hour_cos, etc.\n",
    "df = add_heating_duration(df)  # Creates heating_duration_min\n",
    "\n",
    "# 3. Now prepare for model\n",
    "(X_train, y_train), (X_test, y_test), scaler = prepare_data(df)\n",
    "\n",
    "# 4. Verify the data\n",
    "print(f\"Total sequences: {len(X_train)} training, {len(X_test)} test\")\n",
    "print(f\"Each sequence shape: {X_train[0].shape}\")  # Should be (history_length, num_features)\n",
    "\n",
    "# 5. Inspect a sample\n",
    "sample_idx = 500\n",
    "try:\n",
    "    print(\"\\nSample Input Features Shape:\", X_train[sample_idx].shape)\n",
    "    print(\"\\nFirst Timestep Features:\")\n",
    "    print(pd.DataFrame([X_train[sample_idx][0]], \n",
    "                      columns=['indoor_temp', 'humidity', 'exhaust_temp', \n",
    "                              'solar_rad', 'outdoor_temp', 'hour_sin', \n",
    "                              'hour_cos', 'heating_dur_min', 'heating_status', \n",
    "                              'is_weekend']))\n",
    "    \n",
    "    print(\"\\nHeating Duration Evolution:\")\n",
    "    print(X_train[sample_idx][:, -3])  # 3rd last feature\n",
    "    \n",
    "    print(\"\\nCorresponding Target Temperature:\", y_train[sample_idx])\n",
    "except IndexError:\n",
    "    print(f\"Sample {sample_idx} doesn't exist. Max index is {len(X_train)-1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
